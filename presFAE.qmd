---
title: "Análise de Dados do Mercado Financeiro:"
subtitle: "***utilizando R ou Python com ferramentas de Big Data***"
author: <a href="https://rhozon.github.io/selecaodeprodutosnor/index.html">Rodrigo Hermont Ozon</a>
author-meta: "<a href='https://rhozon.github.io/selecaodeprodutosnor/index.html'>Rodrigo Hermont Ozon</a>"
date: "29-nov-2024"
title-slide-attributes:
    data-background-image: "https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png"
    data-background-size: contain
    data-background-opacity: "number"
format:
  revealjs:
    incremental: true   
    slide-number: true
    chalkboard: 
      buttons: true
    preview-links: auto
    css: stylespres.css
    subtitle-font-size: 8
---

```{css, include=FALSE}

.title {
  font-size: 60px !important;
  color: red !important;
}

.author {
  font-size: 50px;
  color: blue;
}

.date {
  font-size: 50px;
  color: green;
}

```

```{r setup, include=FALSE}

start_time <- Sys.time()

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	comment = NA
)
knitr::opts_chunk$set(comment = NA)    # Remove all coments # of R outputs
knitr::opts_chunk$set(warning = FALSE) # Remove all warnings # of R outputs
knitr::opts_chunk$set(message = FALSE) # Remove all messages # of R outputs

```

## Introdução {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

 

<font size = 5>

-   **Mercado financeiro**:
    -   É um ambiente dinâmico onde decisões baseadas em dados podem otimizar resultados.
-   **Data driven decision making**:
    -   Como tomar uma decisão econômica de maneira rápida, ótima e eficiente ?

::: {.callout-caution collapse="false" appearance="simple"}
## Por que Big Data é relevante ?

-   Volume de Dados: A análise do mercado de commodities envolve um enorme volume de dados financeiros e econômicos que cresce rapidamente, especialmente considerando históricos de preços diários, transações e eventos externos que influenciam os mercados.

-   Variabilidade e Velocidade: As séries de preços de commodities possuem mudanças rápidas e variações, exigindo a capacidade de processar e analisar dados em tempo hábil para garantir insights relevantes.

-   Análises em Profundidade: Necessidade de descobrir padrões ocultos que requerem recursos computacionais consideráveis para manipular e analisar um volume grande e complexo de dados.
:::

</font>

## Problemática de Pesquisa {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

```{r echo=FALSE}

library(plotly)
library(fpp3)
library(tsibble)
library(dplyr)
library(quantmod)
library(gridExtra)

tickers <- c(
         "ZC=F", # Futuros Milho
#         "ZO=F", # Futuros Aveia
#         "KE=F", # Futuros KC HRW Wheat Futures
#         "ZR=F", # Rough Rice Futures
#         "GF=F", # Feeder Cattle Futures
#         "ZS=F", # Futuros oleo de soja
#         "ZM=F",  # Futuros farelo soja
         "ZL=F" # Futuros Soja
)

# Calcula os retornos diários:

portfolioPrices <- NULL
  for ( Ticker in tickers )
    portfolioPrices <- cbind(
      portfolioPrices, 
      getSymbols.yahoo(
        Ticker,
        from = "2019-01-01",
        auto.assign = FALSE
      )[,4]
    )

# Limpa todas as datas sem preços:
portfolioPrices <- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]

# Renomeio as colunas
colnames(portfolioPrices) <- tickers

#tail(portfolioPrices)

data_df <- data.frame(
                      time = index(portfolioPrices), 
                      coredata(portfolioPrices)
)

#glimpse(data_df)

# Quebras 2021-05-07 
# 2022-04-18
# 2022-06-30
```

```{r, echo=FALSE }

milho <- ggplotly(
data_df |> ggplot(aes(x = time,
                      y = ZC.F )) +
  geom_line() +
    geom_vline(xintercept = as.numeric(as.Date(c("2021-05-07", "2022-04-18", "2022-06-30"))),
               linetype = "dashed",
               color = "red",
               alpha = 0.5) +
  theme(plot.title = element_text(size = 9, face = "bold" )) + 
  theme(legend.position = "none") + xlab("") + ylab("US$\bushel") +
  theme(axis.title.y = element_text(size = 9)) +
  ggtitle("Evolução dos valores dos preços dos contratos futuros de milho na CBOT") +
  labs(  
  caption = "Source: Data extraction from Yahoo!Finances API"
 )
)

soja <- ggplotly(
data_df |> ggplot(aes(x = time,
                      y = ZL.F )) +
  geom_line() +
    geom_vline(xintercept = as.numeric(as.Date(c("2021-05-07", "2022-04-18", "2022-06-30"))),
               linetype = "dashed",
               color = "red",
               alpha = 0.5) +
  theme(plot.title = element_text(size = 9, face = "bold" )) + 
  theme(legend.position = "none") + xlab("") + ylab("US$\bushel") +
  theme(axis.title.y = element_text(size = 9)) +
  ggtitle("Evolução dos valores dos preços dos contratos futuros de milho e de soja na CBOT (em US$/bushel)") +
  labs(  
  caption = "Source: Data extraction from Yahoo!Finances API"
 )
)

subplot(milho, soja, nrows = 2, shareX = TRUE)

```

<font size = -1>

**Source:** Data extraction from [Yahoo!Finances API](https://cran.r-project.org/web/packages/quantmod/quantmod.pdf) ploted by using [Plotly](https://plotly.com/)

</font>

## Problemática de Pesquisa {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

 

<font size = 6>

::: {.callout-important icon="false"}
## Perguntas

-   Quais as causas ou desencadeadores desses movimentos repentinos de inversão de tendência nas séries de preços ?

-   É possível estimar/medir o quanto essas mudanças bruscas de tendência geram de impacto na economia e no mercado de *commodities* agrícolas ?

-   Como podemos antecipar/prever o acontecimento dessas "quebras" nas séries temporais de preços no futuro ?

-   Pode-se otimizar o processo decisório de compra e venda de grãos recomendando as melhores alocações de portfólio de *commodities* em condições de risco e incerteza ?

-   Como decidir ou facilitar o processo decisório humano dentro de um contexto de massivo volume e velocidade de informações ?

-   Qual o volume de dados ou tamanho de série histórica necessária pra construir um modelo eficiente ?
:::

</font>

## Teorias de Base {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

<font size = 5>

::: {.callout-note appearance="simple" icon="false"}
| Área de Ciência | Teoria | Pensadores |
|------------------|-------------------------|------------------------------|
| Microeconomia | Teoria da Demanda e do Consumidor | Walrás, Pareto, Arrow, Debreu, Samuelson, Hicks |
| Microeconomia | Estruturas de Mercado | Porter, Chamberlin, Joan Robinson, Bain |
| Microeconomia | Finanças Comportamentais | Daniel Kahneman, Amos Tversky, Robert Shiller |
| Microeconomia | Eficiência de Mercado | Eugene Fama, Fischer Black e Myron Scholes, Jensen |
| Microeconomia | Teoria do Portfólio | Harry Markowitz, Milton Friedman, Keynes |
| Finanças | Teoria dos Ciclos Financeiros | Hyman Minsky, Irving Fischer, Joseph Schumpeter e Kondratiev |
| Finanças | Teoria do Mais Tolo (ou Teoria do Toque de Midas Reverso) | John Kenneth Galbraith, Nassim Taleb |
| Econometria Financeira | Bayesian GARCH with Markov Regime Switching | David Ardia, Robert Engle, Tim Bollerslev, Gary Koop |
| Macroeconomia | Teoria da Formação das Expectativas | Robert Lucas, Milton Friedman, Edmund Phelps, Franco Modigliani |
| Neuroeconomia | Teoria da Hipótese da Antecipação de Recompensa | Wolfram Schultz, Antonio Rangel, Paul Glimcher |
| Microeconomia | Teoria da Seleção Adversa | George Akerlof, Michael Spence, Stiglitz |
| Complexidade (Física de redes) | Sistemas Dinâmicos Adpatativos não-lineares | Arthur Ávila, Brian Arthur, Robert May |
:::

</font>

## Hipóteses Científicas {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

<font size = 6>

::: callout-tip
## Insights

-   *Volatility Clustering* e mudanças estruturais

 

-   Análise de Intervenção Causal em Séries Temporais nas quebras e "efeito disseminação"

 

-   Desenvolvimentos do modelo de otimização de portfolio de Markowitz (CAPM, B&S, Merton, Black-Litterman ...)

 

-   Múltiplos Objetivos variando conforme o contexto de mercado e as expectativas percebidas (risco e incerteza)
:::

</font>

## Justificativa e relevância {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

 

<font size = 5>

::: {.callout-caution collapse="false" appearance="simple"}
## Contribuições teóricas

-   Identificação dos *drivers* dos preços auxilia na investigação da causa dos movimentos repentinos nas séries de preços (Teoria da Demanda do Consumidor, Estruturas de Mercado e Teoria do Portfólio \[motivo transação, especulação ou precaução\]) pode ser utilizada em conjunto com a técnica Econométrica de Análise de Intervenção em Séries Temporais \[Angrist e Imbens, Brodersen *et. alli* (2015)\] para avaliar seu impacto causal na série de preço estudada;

-   A Teoria dos Ciclos Financeiros ajuda a compreender em qual contexto econômico a disseminação de efeito econômico-financeiro nocivo ou positivo está inserida frente a quebra repentina da tendência da trajetória de preços de alimentos (*commodities*)

-   O uso das técnicas pertinentes dentro da teoria da Econometria Financeira com o uso do modelo Bayesiano GARCH com mudanças de regime markovianos se mostra aderente à realidade dos dados e condizente com os últimos desenvolvimentos teóricos a respeito do fenômeno da dinâmica complexa dos preços dessas *commodities*;

-   A teoria de alocação de portfólio desde Markowitz pode ser melhor elaborada combinando as ferramentas de otimização multiobjetivo multiperíodo de maneira dinâmica em consonância com modelos econométricos que consigam incorporar com maior clareza a "incerteza" percebida pelos *players* de mercado na sua aferição de risco x retorno. Assim, os processos decisórios de compra e venda em momentos oportunos se tornariam mais claros.
:::

</font>

## Big Data na Análise de Commodities {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

 

<font size = 5>

::: {.callout-tip appearance="simple" icon="false"}
## Amostragem de Dados Significativa

-   Estratégia de Amostragem: Utilizar amostragem estratificada para garantir que a variabilidade ao longo do tempo seja capturada de forma adequada (como choques econômicos ou eventos climáticos que impactam os preços).

-   Redução de Dimensionalidade: Utilizar técnicas de PCA (Principal Component Analysis) para reduzir o número de variáveis sem perder informações importantes, permitindo uma análise mais eficiente dos dados.

-   Uso de Dados Representativos: A seleção de um subconjunto representativo de dados pode ser feita para capturar as tendências de mercado de diferentes períodos, garantindo que os insights gerados sejam válidos e aplicáveis.
:::

</font>

## Big Data na Análise de Commodities {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

 

<font size = 5>

::: {.callout-tip appearance="simple" icon="false"}
## Ferramentas e Bibliotecas Utilizadas (alguns exemplos)

Python:

-   `PySpark`: Uma ferramenta poderosa para processamento distribuído e análise de grandes volumes de dados, ideal para trabalhar com dados de commodities de históricos extensivos.

-   `Dask`: Alternativa à biblioteca Pandas, que facilita o processamento de grandes datasets que não cabem na memória. Dask permite a execução de operações paralelas, otimizando análises.

R:

-   `sparklyr` e `SparkR`: Integração do Spark no ambiente R, possibilitando o processamento distribuído e a manipulação eficiente de datasets gigantescos, com foco em análises financeiras.

-   `vroom` e `data.table`: Utilizadas para leitura rápida e manipulação de grandes volumes de dados armazenados em arquivos CSV, permitindo o carregamento de arquivos grandes em poucos segundos.

\*Existem algumas outras pra R e Python que poderiam ser mencionadas aqui, mas por questões de parcimônia limitaremos a pequenos exemplos.
:::

</font>

## Big Data com R {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

 

<font size = 5>

**Análise de Commodities com `sparklyr` e `vroom`**

```{r, eval = FALSE}

library(sparklyr)
library(dplyr)
library(vroom)

# Conectar ao Spark
sc <- spark_connect(master = "local[*]") # Você pode rodar também no Databricks, Azure, AWS, Google Colab...

# Ler grandes volumes de dados usando vroom
file_list <- list.files("data", pattern = "*_data.csv", full.names = TRUE)
combined_data <- vroom(file_list, col_types = list(
  Date = col_date(),
  Open = col_double(),
  High = col_double(),
  Low = col_double(),
  Close = col_double(),
  Volume = col_double(),
  Adj.Close = col_double(),
  Ticker = col_character()
))

# Copiar os dados para o Spark
spark_data <- copy_to(sc, combined_data, "commodities_data", overwrite = TRUE)

# Executar análise no Spark: calcular a média de fechamento ajustado por commodity
average_close <- spark_data |>
  group_by(Ticker) |>
  summarise(Average_Close = mean(Adj.Close, na.rm = TRUE)) |>
  collect()

print(average_close)

# Desconectar do Spark
spark_disconnect(sc)

```

</font>

## Big Data com Python {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

 

<font size = 5>

**Análise de Commodities com `PySpark` e `Dask`**

```{python}
#| eval: false

from pyspark.sql import SparkSession
import dask.dataframe as dd

# Inicializar a sessão Spark
spark = SparkSession.builder.master("local").appName("Commodities Analysis").getOrCreate()

# Carregar dados grandes de commodities usando Dask
file_list = ["data/ZC=F_data.csv", "data/ZO=F_data.csv", "data/KE=F_data.csv"]
df_dask = dd.read_csv(file_list)

# Converter o DataFrame Dask para Spark
df_spark = spark.createDataFrame(df_dask.compute())

# Analisar os dados no Spark
df_spark.createOrReplaceTempView("commodities_data")
result = spark.sql("SELECT Ticker, AVG(Adj_Close) as Average_Close FROM commodities_data GROUP BY Ticker")
result.show()

# Finalizar a sessão Spark
spark.stop()

```

</font>

## Exercícios rápidos {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

 

<font size = 4>

1.  A partir das instruções contidas no post em [como ler com o Sparklyr Big Data localmente](https://rhozon.github.io/selecaodeprodutosnor/sparklyr.html), leia e processe usando o pacote do R `sparklyr` localmente o conjunto de dados [vix-daily.csv (menor)](https://github.com/rhozon/Doutorado/blob/main/vix-daily.csv) e o .rds [gbt-model-responses](https://github.com/rhozon/Doutorado/blob/main/gbt-model-responses.rds). 1.a) Agora leia eles usando o `Dask` (Python), `data.table` e `vroom` (R) localmente e as reporte num documento [Quarto](https://quarto.org/)

2.  Como você analisaria o conjunto de dados de [hipotecas (mortgages)](https://assets.datacamp.com/production/repositories/721/datasets/c07698e46226e841b9cdb1ad0248c664c2c20bbb/mortgage-sample.csv) [ou outra fonte aqui no repo](https://github.com/rhozon/Doutorado/blob/main/mortgage-sample.csv) ? Ele contém dados fictícios ou anonimizados sobre empréstimos imobiliários, fornecendo informações que podem ser usadas para estudar padrões de empréstimos, análise de risco de crédito ou previsões de inadimplência. Elabore uma proposta de projeto a partir desse dataset em R ou em Python utilizando o report utilizando o [quarto.org](https://quarto.org/) pra ganhar velocidade!

3.  E quando o Spark, `data.table`, `vroom`, `Dask` e etc. não derem mais conta do recado ? Pesquise alternativas e sugira algumas estratégias pra lidar com grandes volumes de dados e modelá-los ! Lembre-se de citar as fontes, pacotes, métodos e referências necessárias. (Algumas sugestões de uso de dados brasileiros via extração por API em [Repo Doc. gitHub](https://github.com/leonardogrig/dados-mercado-financeiro-brasil))

4.  Expanda o exemplo do portfólio de commodities, utilizando uma extração de séries temporais mais longas no tempo, inserindo mais ativos no portfólio (utilize o pacote do R `quantmod` ou do Python `yfinance`) utilizando a sintaxe do Spark e experimente os pacotes do R úteis pra isso como a sintaxe do `data.table` ou a leitura do conjunto de dados com o `vroom` localmente. Como poderemos analisar tendências desses dados utilizando um modelo de médias móveis simples ?

</font>

## Mais informações e referências {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

 

<font size = 6>

::: {.callout-note appearance="simple"}
-   Envie esses reports nos formatos .qmd e .html num arquivo .zip para [rodrigo.ozon\@fae.edu.br](mailto:rodrigo.ozon@fae.edu.br)

-   Prazo de envio (1 e 2 até o final da próxima semana) e 3 e 4 (até o final da semana subsequente)

-   Maiores detalhes a respeito de fontes, referências, recomendações de cursos online, pacotes e libs, consulte por favor o Plano de Aula de hoje
:::

</font>

## Obrigado! {background-color="white" background-image="https://raw.githubusercontent.com/rhozon/Doutorado/main/fae_bckground.png" background-size="1500px" background-repeat="no-repeat"}

 

<font size = 6>

::: {.callout-note appearance="simple"}
## Rodrigo Hermont Ozon

$\Rightarrow$ Agradecimentos à todos os membros da banca examinadora e demais ouvintes:

-   Meu perfil no [Google Scholar](https://scholar.google.com/citations?user=hPcIR9oAAAAJ&hl=en)
-   Meu [CV Lattes](http://lattes.cnpq.br/3532649625879285)
-   Meu [site com posts, tutoriais e artigos](https://rhozon.github.io/selecaodeprodutosnor/index.html)
-   Meu [perfil no LinkeDin](https://www.linkedin.com/in/rodrigohermontozon/)
:::

 

 

```{=html}

<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

"Situations emerge in the process of creative destruction in which many firms may have to perish that nevertheless would be able to live on vigorously and usefully if they could weather a particular storm.

[... Capitalism requires] the perennial gale of Creative Destruction." Schumpeter, Joseph A. (1994) [1942]. Capitalism, Socialism and Democracy. London: Routledge. pp. 82–83. ISBN 978-0-415-10762-4. Retrieved 23 November 2011. 

</p>
```

</font>
